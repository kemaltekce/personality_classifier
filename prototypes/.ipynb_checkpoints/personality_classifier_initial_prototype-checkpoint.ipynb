{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Personality:\n",
    "    FIRST_PERSONALITY = 'Introvert'\n",
    "    SECOND_PERSONALITY = 'Extrovert'\n",
    "\n",
    "    \n",
    "class Persona:\n",
    "    def __init__(self, personality, texts):\n",
    "        self.personality = self._assign_personality(personality)\n",
    "        self.texts = self._assign_texts(texts)\n",
    "        \n",
    "    def _assign_personality(self, personality):\n",
    "        # first letter identifies if a person is an\n",
    "        # introvert or extrovert\n",
    "        persona = personality[0]\n",
    "        message = 'Personality of I (Introvert) or E (Extrovert) is expected.'\n",
    "        assert persona in ('I', 'E'), message\n",
    "        if persona == 'I':\n",
    "            return 'Introvert'\n",
    "        elif persona == 'E':\n",
    "            return 'Extrovert'\n",
    "            \n",
    "    def _assign_texts(self, texts):\n",
    "        if '|||' in texts:\n",
    "            return texts.split('|||')\n",
    "        elif isinstance(texts, list):\n",
    "            return texts\n",
    "        else:\n",
    "            raise ValueError('Can not handle the texts input.')\n",
    "\n",
    "\n",
    "class PersonaContainer:\n",
    "    def __init__(self, personas):\n",
    "        self.personas = personas\n",
    "        \n",
    "    def get_personality(self):\n",
    "        return [x.personality for x in self.personas]\n",
    "    \n",
    "    def get_texts(self):\n",
    "        return [x.texts for x in self.personas]\n",
    "    \n",
    "    def evenly_distribute(self):\n",
    "        first = list(filter(\n",
    "            lambda x: x.personality == Personality.FIRST_PERSONALITY, self.personas))\n",
    "        second = list(filter(\n",
    "            lambda x: x.personality == Personality.SECOND_PERSONALITY, self.personas))\n",
    "        if len(first) > len(second):\n",
    "            first_small = first[:len(second)]\n",
    "            self.personas = second + first_small\n",
    "        elif len(first) < len(second):\n",
    "            second_small = second[:len(first)]\n",
    "            self.personas = first + second_small\n",
    "        else:\n",
    "            print(\"Both personalities already have an even distribution.\")\n",
    "        random.shuffle(self.personas)\n",
    "        \n",
    "    def _chunks(self, lst, n):\n",
    "        \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "        \n",
    "    def split_texts(self, chunk_size):\n",
    "        new_personas = []\n",
    "        ignored_text = 0\n",
    "        for persona in self.personas:\n",
    "            texts = persona.texts\n",
    "            personality = persona.personality\n",
    "            for chunk in self._chunks(texts, chunk_size):\n",
    "                if len(chunk) == chunk_size:\n",
    "                    new_personas.append(Persona(personality, chunk))\n",
    "                else:\n",
    "                    ignored_text += len(chunk)\n",
    "        if ignored_text != 0:\n",
    "            print(\"Ignored %d texts because of too small chunks\" % (ignored_text))\n",
    "        self.personas = new_personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/mbti_1.csv') as file:\n",
    "    data = csv.reader(file, delimiter=',')\n",
    "    # ignore header\n",
    "    next(data, None)\n",
    "    personas = []\n",
    "    for row in data:\n",
    "        if '|||' in row[1][1:-1]:\n",
    "            personas.append(Persona(row[0], row[1][1:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Introvert': 6676, 'Extrovert': 1998})\n"
     ]
    }
   ],
   "source": [
    "persona_cont = PersonaContainer(personas)\n",
    "print(Counter(persona_cont.get_personality()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate more data by splitting texts into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored 5624 texts because of too small chunks\n",
      "Counter({'Introvert': 32111, 'Extrovert': 9611})\n"
     ]
    }
   ],
   "source": [
    "persona_cont.split_texts(10)\n",
    "print(Counter(persona_cont.get_personality()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribute data evenly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Introvert': 9611, 'Extrovert': 9611})\n"
     ]
    }
   ],
   "source": [
    "persona_cont.evenly_distribute()\n",
    "print(Counter(persona_cont.get_personality()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(\n",
    "    persona_cont.personas, test_size=0.3, random_state = 123, \n",
    "    stratify=persona_cont.get_personality())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Introvert': 6728, 'Extrovert': 6727})\n",
      "Counter({'Extrovert': 2884, 'Introvert': 2883})\n"
     ]
    }
   ],
   "source": [
    "train_cont = PersonaContainer(train)\n",
    "print(Counter(train_cont.get_personality()))\n",
    "\n",
    "test_cont = PersonaContainer(test)\n",
    "print(Counter(test_cont.get_personality()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [' '.join(x) for x in train_cont.get_texts()]\n",
    "train_y = train_cont.get_personality()\n",
    "\n",
    "test_x = [' '.join(x) for x in test_cont.get_texts()]\n",
    "test_y = test_cont.get_personality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(max_df=0.8, min_df=0.)\n",
    "train_x_vect = vect.fit_transform(train_x)\n",
    "test_x_vect = vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kemaltekce/miniconda3/envs/sklearn/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=123, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "clf_log = LogisticRegression(random_state=123)\n",
    "clf_log.fit(train_x_vect, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.717357378186232\n"
     ]
    }
   ],
   "source": [
    "print(clf_log.score(test_x_vect, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72778891 0.70609448]\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(\n",
    "    test_y, clf_log.predict(test_x_vect), average=None, \n",
    "    labels=[Personality.FIRST_PERSONALITY, Personality.SECOND_PERSONALITY]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2179  704]\n",
      " [ 926 1958]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(\n",
    "    test_y, clf_log.predict(test_x_vect), \n",
    "    labels=[Personality.FIRST_PERSONALITY, Personality.SECOND_PERSONALITY])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with own posts\n",
    "\n",
    "TODO\n",
    "- build own predict class which checks if words are in vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have a look at this -> Introvert\n",
      "\n",
      "Yes I love all of you so much -> Extrovert\n",
      "\n",
      "I can not classify the post because do not know any word in the post: blub\n",
      "\n",
      "I had a wonderful day with my friends -> Extrovert\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts = [\"Have a look at this\", \"Yes I love all of you so much\", \"blub\", \"I had a wonderful day with my friends\"]\n",
    "for post in posts:\n",
    "    if set(post.split(' ')).intersection(set(vect.get_feature_names())):\n",
    "        pred = clf_log.predict(vect.transform([post]))[0]\n",
    "        print(post + ' -> ' + pred)\n",
    "    else:\n",
    "        print('I can not classify the post because do not know any word in the post: ' + post)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
