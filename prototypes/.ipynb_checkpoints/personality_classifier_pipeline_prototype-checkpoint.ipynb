{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hatstall.pipes.loader import (\n",
    "    Personality, PersonalityPostLoader)\n",
    "from hatstall.pipes.preparator import (\n",
    "    PersonContainer, PostsSplitter, EvenlyDistributor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PersonalityPostLoader(payload)\n",
    "splitter = PostsSplitter(payload)\n",
    "evenfier = EvenlyDistributor(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check amount of data and distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Introvert': 6676, 'Extrovert': 1998})\n"
     ]
    }
   ],
   "source": [
    "person_cont = PersonContainer(payload['persons'])\n",
    "print(Counter(person_cont.get_personality()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate more data by splitting posts into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored 5624 texts because of too small chunks\n"
     ]
    }
   ],
   "source": [
    "splitter.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Introvert': 32111, 'Extrovert': 9611})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(payload['persons_container'].get_personality()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distribute data evenly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "evenfier.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Introvert': 9611, 'Extrovert': 9611})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(payload['persons_container'].get_personality()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engieneering and modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Introvert': 6728, 'Extrovert': 6727})\n",
      "Counter({'Extrovert': 2884, 'Introvert': 2883})\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(\n",
    "    payload['persons_container'].persons, test_size=0.3, \n",
    "    random_state = 123, \n",
    "    stratify=payload['persons_container'].get_personality())\n",
    "\n",
    "train_cont = PersonContainer(train)\n",
    "print(Counter(train_cont.get_personality()))\n",
    "\n",
    "test_cont = PersonContainer(test)\n",
    "print(Counter(test_cont.get_personality()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [' '.join(x) for x in train_cont.get_posts()]\n",
    "train_y = train_cont.get_personality()\n",
    "\n",
    "test_x = [' '.join(x) for x in test_cont.get_posts()]\n",
    "test_y = test_cont.get_personality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7230795907750998\n",
      "[0.73334446 0.71199279]\n",
      "[[2196  687]\n",
      " [ 910 1974]]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(max_df=0.8, min_df=0.)),\n",
    "    ('log', LogisticRegression(random_state=123)),\n",
    "])\n",
    "\n",
    "pipeline.fit(train_x, train_y)\n",
    "\n",
    "print(pipeline.score(test_x, test_y))\n",
    "print(f1_score(\n",
    "    test_y, pipeline.predict(test_x), average=None, \n",
    "    labels=[\n",
    "        Personality.FIRST_PERSONALITY, \n",
    "        Personality.SECOND_PERSONALITY]))\n",
    "cm = confusion_matrix(\n",
    "    test_y, pipeline.predict(test_x), \n",
    "    labels=[\n",
    "        Personality.FIRST_PERSONALITY, \n",
    "        Personality.SECOND_PERSONALITY])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline as ModelPipeline\n",
    "\n",
    "from hatstall.pipes import Pipeline as CustomPipeline, PipelineSystem\n",
    "from hatstall.pipes.engineer import PostsJoiner\n",
    "from hatstall.pipes.model import Evaluator\n",
    "from hatstall.pipes.loader import PersonalityPostLoader\n",
    "from hatstall.pipes.preparator import PostsSplitter, EvenlyDistributor, TrainTestSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running preparation pipeline ---\n",
      "Running loader pipe --> PersonalityPostLoader\n",
      "Running splitter pipe --> PostsSplitter\n",
      "Ignored 5624 texts because of too small chunks\n",
      "Running evenfier pipe --> EvenlyDistributor\n",
      "Running traintest pipe --> TrainTestSplitter\n",
      "Train size: Counter({'Introvert': 6728, 'Extrovert': 6727})\n",
      "Test size: Counter({'Extrovert': 2884, 'Introvert': 2883})\n",
      "--- Running modelling pipeline ---\n",
      "--- Running evaluation pipeline ---\n",
      "Running eval pipe --> Evaluator\n",
      "0.7194381827640021\n",
      "[0.7295219  0.70857349]\n",
      "[[2182  701]\n",
      " [ 917 1967]]\n"
     ]
    }
   ],
   "source": [
    "PreparationPipeline = CustomPipeline\n",
    "EvaluationPipeline = CustomPipeline\n",
    "\n",
    "psystem = PipelineSystem([\n",
    "    ('preperation', PreparationPipeline([\n",
    "        ('loader', PersonalityPostLoader),\n",
    "        ('splitter', PostsSplitter),\n",
    "        ('evenfier', EvenlyDistributor),\n",
    "        ('traintest', TrainTestSplitter)\n",
    "    ])),\n",
    "    ('modelling', ModelPipeline([\n",
    "        ('joiner', PostsJoiner()),\n",
    "        ('vect', TfidfVectorizer(max_df=0.8, min_df=0.)),\n",
    "        ('log', LogisticRegression(random_state=123)),\n",
    "    ])),\n",
    "    ('evaluation', EvaluationPipeline([\n",
    "        ('eval', Evaluator)\n",
    "    ]))\n",
    "], mode='train_test')\n",
    "psystem.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- feature uninon input list of messages\n",
    "  - calculate average word in message\n",
    "  - check if link in message\n",
    "  - combine messages and use vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
